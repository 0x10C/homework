{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pretreatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz, process as fuzzy_process\n",
    "import re\n",
    "import numpy as np\n",
    "from bisect import bisect_left\n",
    "import torch\n",
    "\n",
    "def fuzzy_retrieve(entity, pool, setting, threshold = 50):\n",
    "    \"\"\"Fuzzily match the exact name of the entity.\n",
    "\n",
    "    The exacted name in text might be slightly different from the name in its wiki page. \n",
    "    A simple fuzzy matching with names in links can solve this problem.\n",
    "    But note that as HotpotQA paper claims, the fullwiki dataset has maintained the consistence. \n",
    "    \n",
    "    Args:\n",
    "        entity (string): The entity name exacted from the text.\n",
    "        pool (tuple): For fullwiki setting, is a (title, sentence number) tuple. \n",
    "        setting (string): setting.\n",
    "        threshold (int, optional): Matching threshold. Defaults to 50.\n",
    "    \n",
    "    Returns:\n",
    "        string: Matched name.\n",
    "    \"\"\"\n",
    "    if setting == 'distractor':\n",
    "        pool = pool.keys()\n",
    "    else:\n",
    "        if not hasattr(fuzzy_retrieve, 'db'):\n",
    "            from redis import StrictRedis\n",
    "            fuzzy_retrieve.db = StrictRedis(host='localhost', port=6379, db=0)\n",
    "        assert isinstance(pool, tuple)\n",
    "        title, sen_num = pool\n",
    "        pool = set()\n",
    "        for i in range(sen_num + 1):\n",
    "            name = 'edges:###{}###{}'.format(i, title)\n",
    "            tmp = set([x.decode().split('###')[0] for x in fuzzy_retrieve.db.lrange(name, 0, -1)])\n",
    "            pool |= tmp\n",
    "        \n",
    "    best = (0, -1)\n",
    "    for item in pool:\n",
    "        item_refined = re.sub(r' \\(.*?\\)$', '', item)\n",
    "        score = fuzz.ratio(item_refined, entity)\n",
    "        if best[0] < score:\n",
    "            best = (score, item)\n",
    "    return best[1] if best[0] > threshold else None\n",
    "\n",
    "def get_context_fullwiki(title):\n",
    "    \"\"\"Fetch the sentences of the page about \"title\".\n",
    "    \n",
    "    Args:\n",
    "        title (string): Entity name.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of sentences(string). \n",
    "    \"\"\"\n",
    "    if not hasattr(get_context_fullwiki, 'db'):\n",
    "        from redis import StrictRedis\n",
    "        get_context_fullwiki.db = StrictRedis(host='localhost', port=6379, db=0, decode_responses=True)\n",
    "    return get_context_fullwiki.db.lrange(title, 0, -1)\n",
    "\n",
    "def dp(a, b):\n",
    "    \"\"\"A basic Dynamic programming for Edit-distance based fuzzy matching.\n",
    "    \n",
    "    Args:\n",
    "        a (string): source.\n",
    "        b (string): the long text.\n",
    "    \"\"\"\n",
    "    f, start = np.zeros((len(a), len(b))), np.zeros((len(a), len(b)), dtype = np.int)\n",
    "    for j in range(len(b)):\n",
    "        f[0, j] = int(a[0] != b[j])\n",
    "        if j > 0 and b[j - 1].isalnum():\n",
    "            f[0, j] += 10 \n",
    "        start[0, j] = j\n",
    "    for i in range(1, len(a)):        \n",
    "        for j in range(len(b)):\n",
    "            # (0, i-1) + del(i) ~ (start[j], j)\n",
    "            f[i, j] = f[i - 1, j] + 1\n",
    "            start[i, j] = start[i - 1, j]\n",
    "            if j == 0:\n",
    "                continue\n",
    "            if f[i, j] > f[i - 1, j - 1] + int(a[i] != b[j]):\n",
    "                f[i, j] = f[i - 1, j - 1] + int(a[i] != b[j])\n",
    "                start[i, j] = start[i-1, j - 1]\n",
    "\n",
    "            if f[i, j] > f[i, j - 1] + 0.5:\n",
    "                f[i, j] = f[i, j - 1] + 0.5\n",
    "                start[i, j] = start[i, j - 1]\n",
    "    r = np.argmin(f[len(a) - 1])\n",
    "    ret = [start[len(a) - 1, r], r + 1]\n",
    "    score = f[len(a) - 1, r] / len(a)\n",
    "    return (ret, score)\n",
    "\n",
    "def fuzzy_find(entities, sentence, ratio = 80):\n",
    "    \"\"\"Try to find as much entities in sentence precisely.\n",
    "\n",
    "    Args:\n",
    "        entities (list): Candidates.\n",
    "        sentence (string): The sentence to examine.\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: (entity, match span, start position, end position, score)\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    for entity in entities:\n",
    "        item = re.sub(r' \\(.*?\\)$', '', entity).strip()\n",
    "        if item == '':\n",
    "            item = entity\n",
    "            print(item)\n",
    "        r, score = dp(item, sentence)\n",
    "        if score < 0.5:\n",
    "            matched = sentence[r[0]: r[1]].lower()\n",
    "            final_word = item.split()[-1]\n",
    "            retry = False\n",
    "            while fuzz.partial_ratio(final_word.lower(), matched) < ratio:\n",
    "                retry = True\n",
    "                end = len(item) - len(final_word)\n",
    "                while end > 0 and item[end - 1].isspace():\n",
    "                    end -= 1\n",
    "                if end == 0:\n",
    "                    retry = False\n",
    "                    score = 1\n",
    "                    break\n",
    "                item = item[:end]\n",
    "                final_word = item.split()[-1]\n",
    "            if retry:\n",
    "                r, score = dp(item, sentence)\n",
    "                score += 0.1\n",
    "            if score >= 0.5:\n",
    "                continue\n",
    "            del final_word\n",
    "            # from start\n",
    "            retry = False\n",
    "            first_word = item.split()[0]\n",
    "            while fuzz.partial_ratio(first_word.lower(), matched) < ratio:\n",
    "                retry = True\n",
    "                start = len(first_word)\n",
    "                while start < len(item) and item[start].isspace():\n",
    "                    start += 1\n",
    "                if start == len(item):\n",
    "                    retry = False\n",
    "                    score = 1\n",
    "                    break\n",
    "                item = item[start:]\n",
    "                first_word = item.split()[0]\n",
    "            if retry:\n",
    "                r, score = dp(item, sentence)\n",
    "                score = max(score, 1 - ((r[1] - r[0]) / len(entity)))\n",
    "                score += 0.1\n",
    "            if score < 0.5:\n",
    "                if item.isdigit() and sentence[r[0]: r[1]] != item:\n",
    "                    continue\n",
    "                ret.append((entity, sentence[r[0]: r[1]], int(r[0]), int(r[1]), score))\n",
    "    non_intersection = []\n",
    "    for i in range(len(ret)):\n",
    "        ok = True\n",
    "        for j in range(len(ret)):\n",
    "            if j != i:\n",
    "                if not (ret[i][2] >= ret[j][3] or ret[j][2] >= ret[i][3]) and ret[j][4] < ret[i][4]:\n",
    "                    ok = False\n",
    "                    break\n",
    "                if ret[i][4] > 0.2 and ret[j][4] < 0.1 and not ret[i][1][0].isupper() and len(ret[i][1].split()) <= 3:\n",
    "                    ok = False\n",
    "                    break\n",
    "        if ok:\n",
    "            non_intersection.append(ret[i][:4])\n",
    "    return non_intersection\n",
    "\n",
    "GENERAL_WD = ['is', 'are', 'am', 'was', 'were', 'have', 'has', 'had', 'can', 'could', \n",
    "              'shall', 'will', 'should', 'would', 'do', 'does', 'did', 'may', 'might', 'must', 'ought', 'need', 'dare']\n",
    "GENERAL_WD += [x.capitalize() for x in GENERAL_WD]\n",
    "GENERAL_WD = re.compile(' |'.join(GENERAL_WD))\n",
    "\n",
    "def judge_question_type(q : str, G = GENERAL_WD) -> int:\n",
    "    if q.find(' or ') >= 0:\n",
    "        return 2 \n",
    "    elif G.match(q):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return 1.0 - x\n",
    "\n",
    "\n",
    "def find_start_end_after_tokenized(tokenizer, tokenized_text, spans: ['Obama Care', '2006']):\n",
    "    \"\"\"Find start and end positions of untokenized spans in tokenized text.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer (Tokenizer): Word-Piece tokenizer.\n",
    "        tokenized_text (list): List of word pieces(string). \n",
    "        spans (list): list of untokenized spans(string).\n",
    "    \n",
    "    Returns:\n",
    "        list: List of (start position, end position).\n",
    "    \"\"\"\n",
    "    end_offset, ret = [], []\n",
    "    for x in tokenized_text:\n",
    "        offset = len(x) + (end_offset[-1] if len(end_offset) > 0 else -1)\n",
    "        end_offset.append(offset)\n",
    "    text = ''.join(tokenized_text)\n",
    "    for span in spans:\n",
    "        t = ''.join(tokenizer.tokenize(span))\n",
    "        start = text.find(t)\n",
    "        if start >= 0:\n",
    "            end = start + len(t) - 1 # include end\n",
    "        else:\n",
    "            result = fuzzy_find([t], text)\n",
    "            if len(result) == 0:    \n",
    "                result = fuzzy_find([re.sub('[UNK]', '',t)], text)\n",
    "                if len(result) == 0:\n",
    "                    raise ValueError('Cannot find an exact match.')\n",
    "            _, _, start, end = result[0]\n",
    "            end -= 1\n",
    "        ret.append((bisect_left(end_offset, start), bisect_left(end_offset, end)))\n",
    "    return ret\n",
    "    \n",
    "def find_start_end_before_tokenized(orig_text, spans: [['Oba', '##ma', 'Care'], ['2006']]):\n",
    "    \"\"\"Find start and end positions of tokenized spans in untokenized text.\n",
    "    \n",
    "    Args:\n",
    "        orig_text (string): Original text.\n",
    "        spans (list): List of list of word pieces, as showed in example.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of (start position, end position).\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    orig_text = orig_text.lower()\n",
    "    for span_pieces in spans:\n",
    "        if len(span_pieces) == 0:\n",
    "            ret.append((0, 0))\n",
    "            continue\n",
    "        span = re.sub('##', '', ''.join(span_pieces))\n",
    "        start = orig_text.find(span)\n",
    "        if start >= 0:\n",
    "            end = start + len(span) # exclude end\n",
    "        else:\n",
    "            result = fuzzy_find([span], orig_text)\n",
    "            if len(result) == 0 and span.find('[UNK]') > 0:\n",
    "                span = span.replace('[UNK]', '')\n",
    "                result = fuzzy_find([span], orig_text)\n",
    "            if len(result) == 0:\n",
    "                ret.append((0,0))\n",
    "                continue\n",
    "            _, _, start, end = result[0]\n",
    "        ret.append((start, end))\n",
    "    return ret\n",
    "\n",
    "def bundle_part_to_batch(all_bundle, l = None, r = None):\n",
    "    \"\"\"Convert all_bundle[l:r] to a batch of inputs.\n",
    "    \n",
    "    Args:\n",
    "        all_bundle (list of Bundles): Data in ``Bundle'' format.\n",
    "        l (int, optional): Left endpoint of the interval. Defaults to None.\n",
    "        r (int, optional): Right endpoint of the interval. Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A batch of inputs.\n",
    "    \"\"\"\n",
    "    if l is None:\n",
    "        l, r = 0, len(all_bundle.ids)\n",
    "    num_samples = r - l\n",
    "    max_length = max([len(x) for x in all_bundle.ids[l:r]])\n",
    "    max_seps = max([len(x) for x in all_bundle.sep_positions[l:r]])    \n",
    "    ids = torch.zeros((num_samples, max_length), dtype = torch.long)\n",
    "    sep_positions = torch.zeros((num_samples, max_seps), dtype = torch.long)\n",
    "    hop_start_weights = torch.zeros((num_samples, max_length))\n",
    "    hop_end_weights = torch.zeros((num_samples, max_length))\n",
    "    ans_start_weights = torch.zeros((num_samples, max_length))\n",
    "    ans_end_weights = torch.zeros((num_samples, max_length))\n",
    "    segment_ids = torch.zeros((num_samples, max_length), dtype = torch.long)\n",
    "    input_mask = torch.zeros((num_samples, max_length), dtype = torch.long)\n",
    "    for i in range(l, r):\n",
    "        length = len(all_bundle.ids[i])\n",
    "        sep_num = len(all_bundle.sep_positions[i])\n",
    "        ids[i - l, :length] = torch.tensor(all_bundle.ids[i], dtype = torch.long)\n",
    "        sep_positions[i - l, :sep_num] = torch.tensor(all_bundle.sep_positions[i])\n",
    "        hop_start_weights[i - l, :length] = torch.tensor(all_bundle.hop_start_weights[i])\n",
    "        hop_end_weights[i - l, :length] = torch.tensor(all_bundle.hop_end_weights[i])\n",
    "        ans_start_weights[i - l, :length] = torch.tensor(all_bundle.ans_start_weights[i])\n",
    "        ans_end_weights[i - l, :length] = torch.tensor(all_bundle.ans_end_weights[i])\n",
    "        segment_ids[i - l, :length] = torch.tensor(all_bundle.segment_ids[i], dtype = torch.long)\n",
    "        input_mask[i - l, :length] = 1\n",
    "    return ids, segment_ids, input_mask, sep_positions, hop_start_weights, hop_end_weights, ans_start_weights, ans_end_weights\n",
    "\n",
    "class WindowMean:\n",
    "    def __init__(self, window_size = 50):\n",
    "        self.array = []\n",
    "        self.sum = 0\n",
    "        self.window_size = window_size\n",
    "    def update(self, x):\n",
    "        self.array.append(x)\n",
    "        self.sum += x\n",
    "        if len(self.array) > self.window_size:\n",
    "            self.sum -= self.array.pop(0)\n",
    "        return self.sum / len(self.array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Reading! len =  90447\n",
      "[('Miami Gardens, Florida', 'Miami Gardens,', 64, 78), ('Hard Rock Stadium', 'Hard Rock Stadium', 0, 17)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/90447 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5a7b23ca554299042af8f703\n",
      "5abed6d45542990832d3a0ef\n",
      "5ab6b2fb5542995eadef0060\n",
      "5ae0e2df5542990adbacf6b1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 19%|█▉        | 17393/90447 [16:40<1:10:00, 17.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|█▉        | 17393/90447 [16:52<1:10:00, 17.39it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5a8d6138554299585d9e37c7\n",
      "5ab740165542992aa3b8c7fa\n",
      "5ab2f812554299545a2cfaee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 39%|███▊      | 34968/90447 [33:20<52:59, 17.45it/s]  \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|███▊      | 34968/90447 [33:33<52:59, 17.45it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5ae7e8ef5542994a481bbe05\n",
      "5ab273ee5542997061209606\n",
      "5a84517355429933447460d5\n",
      "5a7e5b2455429934daa2fc10\n",
      "5a846921554299123d8c2243\n",
      "5add66475542992200553af1\n",
      "5a847c91554299123d8c2268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 58%|█████▊    | 52465/90447 [50:00<36:15, 17.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 58%|█████▊    | 52465/90447 [50:13<36:15, 17.46it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5a7b629555429927d897bfa4\n",
      "5a80577a5542996402f6a4e9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 77%|███████▋  | 69701/90447 [1:06:40<19:52, 17.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|███████▋  | 69701/90447 [1:06:54<19:52, 17.39it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5a8164fb5542995ce29dcbf6\n",
      "5abe4bb855429965af743eb8\n",
      "5a90abc355429933b8a2058a\n",
      "5ab6460b5542995eadeeff96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 96%|█████████▋| 87259/90447 [1:23:20<03:02, 17.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████▋| 87259/90447 [1:23:34<03:02, 17.44it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5a8c7b125542995e66a47614\n",
      "5a8a317455429930ff3c0cef\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90447/90447 [1:26:22<00:00, 17.45it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# %pdb on\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm \n",
    "from pytorch_pretrained_bert.tokenization import whitespace_tokenize, BasicTokenizer, BertTokenizer\n",
    "from redis import StrictRedis\n",
    "# from utils import fuzzy_find\n",
    "\n",
    "\n",
    "db = StrictRedis(host='localhost', port=6379, db=0)\n",
    "\n",
    "\n",
    "with open('./hotpot_train_v1_1.1.json', 'r') as fin:\n",
    "    train_set = json.load(fin)\n",
    "print('Finish Reading! len = ', len(train_set))\n",
    "\n",
    "\n",
    "from hotpot_evaluate_v1 import normalize_answer, f1_score\n",
    "from fuzzywuzzy import fuzz, process as fuzzy_process\n",
    "\n",
    "def fuzzy_retrive(entity, pool):\n",
    "    if len(pool) > 100:\n",
    "        # fullwiki, exact match\n",
    "        # TODO: test ``entity (annotation)'' and find the most like one\n",
    "        if pool.get(entity):\n",
    "            return entity\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        # distractor mode or use link in original wiki, no need to consider ``entity (annotation)''\n",
    "        pool = pool if isinstance(pool, list) else pool.keys()\n",
    "        f1max, ret = 0, None\n",
    "        for t in pool:\n",
    "            f1, precision, recall = f1_score(entity, t)\n",
    "            if f1 > f1max:\n",
    "                f1max, ret = f1, t\n",
    "        return ret\n",
    "\n",
    "def find_near_matches(w, sentence):\n",
    "    ret = []\n",
    "    max_ratio = 0\n",
    "    t = 0\n",
    "    for word in sentence.split():\n",
    "        while sentence[t] != word[0]:\n",
    "            t += 1\n",
    "        score = (fuzz.ratio(w, word) + fuzz.partial_ratio(w, word)) / 2\n",
    "        if score > max_ratio:\n",
    "            max_ratio = score\n",
    "            ret = [(t, t + len(word))]\n",
    "        elif score == max_ratio:\n",
    "            ret.append((t, t + len(word)))\n",
    "        else:\n",
    "            pass\n",
    "        t += len(word)\n",
    "    return ret if max_ratio > 85 else []     \n",
    "\n",
    "print(list(fuzzy_find(['Miami Gardens, Florida', 'WSCV', 'Hard Rock Stadium'], r\"Hard Rock Stadium is a multipurpose football stadium located in Miami Gardens, a city north of Miami. It is the home stadium of the Miami Dolphins of the National Football League (NFL).\")))\n",
    "\n",
    "\n",
    "# construct cognitive graph in training data    \n",
    "from utils import judge_question_type\n",
    "def find_fact_content(bundle, title, sen_num):\n",
    "    for x in bundle['context']:\n",
    "        if x[0] == title:\n",
    "            return x[1][sen_num]\n",
    "test = copy.deepcopy(train_set)\n",
    "for bundle in tqdm(test,mininterval=1000):\n",
    "    entities = set([title for title, sen_num in bundle['supporting_facts']])\n",
    "    bundle['Q_edge'] = fuzzy_find(entities, bundle['question'])\n",
    "    question_type = judge_question_type(bundle['question'])\n",
    "    for fact in bundle['supporting_facts']:\n",
    "        try:\n",
    "            title, sen_num = fact\n",
    "            pool = set()\n",
    "            for i in range(sen_num + 1):\n",
    "                name = 'edges:###{}###{}'.format(i, title)\n",
    "                tmp = set([x.decode().split('###')[0] for x in db.lrange(name, 0, -1)])\n",
    "                pool |= tmp\n",
    "            pool &= entities\n",
    "            stripped = [re.sub(r' \\(.*?\\)$', '', x) for x in pool] + ['yes', 'no']\n",
    "            if bundle['answer'] not in stripped:\n",
    "                if fuzz.ratio(re.sub(r'\\(.*?\\)$', '', title), bundle['answer']) > 80:\n",
    "                    pool.add(title)\n",
    "                else:\n",
    "                    pool.add(bundle['answer'])\n",
    "            if bundle['answer'] == 'yes' or bundle['answer'] == 'no' \\\n",
    "                    or (question_type > 0 and bundle['type'] == 'comparison'):\n",
    "                pool.add(title)\n",
    "            r = fuzzy_find(pool, find_fact_content(bundle, title, sen_num))\n",
    "            fact.append(r)\n",
    "        except IndexError as e: \n",
    "            print(bundle['_id'])\n",
    "with open('./hotpot_train_v1.1_refined.json', 'w') as fout:\n",
    "    json.dump(test, fout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert.modeling import (\n",
    "    BertPreTrainedModel as PreTrainedBertModel, # The name was changed in the new versions of pytorch_pretrained_bert\n",
    "    BertModel,\n",
    "    BertLayerNorm,\n",
    "    gelu,\n",
    "    BertEncoder,\n",
    "    BertPooler,\n",
    ")\n",
    "import torch\n",
    "from torch import nn\n",
    "from utils import (\n",
    "    fuzzy_find,\n",
    "    find_start_end_after_tokenized,\n",
    "    find_start_end_before_tokenized,\n",
    "    bundle_part_to_batch,\n",
    ")\n",
    "from pytorch_pretrained_bert.tokenization import (\n",
    "    whitespace_tokenize,\n",
    "    BasicTokenizer,\n",
    "    BertTokenizer,\n",
    ")\n",
    "import re\n",
    "import pdb\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_sizes, dropout_prob=0.2, bias=False):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(1, len(input_sizes)):\n",
    "            self.layers.append(nn.Linear(input_sizes[i - 1], input_sizes[i], bias=bias))\n",
    "        self.norm_layers = nn.ModuleList()\n",
    "        if len(input_sizes) > 2:\n",
    "            for i in range(1, len(input_sizes) - 1):\n",
    "                self.norm_layers.append(nn.LayerNorm(input_sizes[i]))\n",
    "        self.drop_out = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(self.drop_out(x))\n",
    "            if i < len(self.layers) - 1:\n",
    "                x = gelu(x)\n",
    "                if len(self.norm_layers):\n",
    "                    x = self.norm_layers[i](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def init_weights(self, module):\n",
    "        \"\"\" Initialize the weights.\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.05)\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        super(GCN, self).__init__()\n",
    "        self.diffusion = nn.Linear(input_size, input_size, bias=False)\n",
    "        self.retained = nn.Linear(input_size, input_size, bias=False)\n",
    "        self.predict = MLP(input_sizes=(input_size, input_size, 1))\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, A, x):\n",
    "        layer1_diffusion = A.t().mm(gelu(self.diffusion(x)))\n",
    "        x = gelu(self.retained(x) + layer1_diffusion)\n",
    "        layer2_diffusion = A.t().mm(gelu(self.diffusion(x)))\n",
    "        x = gelu(self.retained(x) + layer2_diffusion)\n",
    "        return self.predict(x).squeeze(-1)\n",
    "\n",
    "\n",
    "class BertEmbeddingsPlus(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, max_sentence_type=30):\n",
    "        super(BertEmbeddingsPlus, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(\n",
    "            config.max_position_embeddings, config.hidden_size\n",
    "        )\n",
    "        self.token_type_embeddings = nn.Embedding(\n",
    "            config.type_vocab_size, config.hidden_size\n",
    "        )\n",
    "\n",
    "        self.sentence_type_embeddings = nn.Embedding(\n",
    "            max_sentence_type, config.hidden_size\n",
    "        )\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(\n",
    "            seq_length, dtype=torch.long, device=input_ids.device\n",
    "        )\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings((token_type_ids > 0).long())\n",
    "        sentence_type_embeddings = self.sentence_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = (\n",
    "            words_embeddings\n",
    "            + position_embeddings\n",
    "            + token_type_embeddings\n",
    "            + sentence_type_embeddings\n",
    "        )\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class BertModelPlus(BertModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertModel, self).__init__(config)\n",
    "        self.embeddings = BertEmbeddingsPlus(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, token_type_ids=None, attention_mask=None, output_hidden=-4\n",
    "    ):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        # We create a 3D attention mask from a 2D tensor mask.\n",
    "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "        # this attention mask is more simple than the triangular masking of causal attention\n",
    "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        extended_attention_mask = extended_attention_mask.to(\n",
    "            dtype=next(self.parameters()).dtype\n",
    "        )  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "        encoded_layers = self.encoder(\n",
    "            embedding_output, extended_attention_mask, output_all_encoded_layers=True\n",
    "        )\n",
    "        sequence_output = encoded_layers[-1]\n",
    "        # pooled_output = self.pooler(sequence_output)\n",
    "        encoded_layers, hidden_layers = (\n",
    "            encoded_layers[-1],\n",
    "            encoded_layers[output_hidden],\n",
    "        )\n",
    "        return encoded_layers, hidden_layers\n",
    "\n",
    "\n",
    "class BertForMultiHopQuestionAnswering(PreTrainedBertModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertForMultiHopQuestionAnswering, self).__init__(config)\n",
    "        self.bert = BertModelPlus(config)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, 4)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        token_type_ids=None,\n",
    "        attention_mask=None,\n",
    "        sep_positions=None,\n",
    "        hop_start_weights=None,\n",
    "        hop_end_weights=None,\n",
    "        ans_start_weights=None,\n",
    "        ans_end_weights=None,\n",
    "        B_starts=None,\n",
    "        allow_limit=(0, 0),\n",
    "    ):\n",
    "        \"\"\" Extract spans by System 1.\n",
    "        \n",
    "        Args:\n",
    "            input_ids (LongTensor): Token ids of word-pieces. (batch_size * max_length)\n",
    "            token_type_ids (LongTensor): The A/B Segmentation in BERTs. (batch_size * max_length)\n",
    "            attention_mask (LongTensor): Indicating whether the position is a token or padding. (batch_size * max_length)\n",
    "            sep_positions (LongTensor): Positions of [SEP] tokens, mainly used in finding the num_sen of supporing facts. (batch_size * max_seps)\n",
    "            hop_start_weights (Tensor): The ground truth of the probability of hop start positions. The weight of sample has been added on the ground truth. \n",
    "                (You can verify it by examining the gradient of binary cross entropy.)\n",
    "            hop_end_weights ([Tensor]): The ground truth of the probability of hop end positions.\n",
    "            ans_start_weights ([Tensor]): The ground truth of the probability of ans start positions.\n",
    "            ans_end_weights ([Tensor]): The ground truth of the probability of ans end positions.\n",
    "            B_starts (LongTensor): Start positions of sentence B.\n",
    "            allow_limit (tuple, optional): An Offset for negative threshold. Defaults to (0, 0).\n",
    "        \n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        batch_size = input_ids.size()[0]\n",
    "        device = input_ids.get_device() if input_ids.is_cuda else torch.device(\"cpu\")\n",
    "        sequence_output, hidden_output = self.bert(\n",
    "            input_ids, token_type_ids, attention_mask\n",
    "        )\n",
    "        semantics = hidden_output[:, 0]\n",
    "        # Some shapes: sequence_output [batch_size, max_length, hidden_size], pooled_output [batch_size, hidden_size]\n",
    "        if sep_positions is None:\n",
    "            return semantics  # Only semantics, used in bundle forward\n",
    "        else:\n",
    "            max_sep = sep_positions.size()[-1]\n",
    "        if max_sep == 0:\n",
    "            empty = torch.zeros(batch_size, 0, dtype=torch.long, device=device)\n",
    "            return (\n",
    "                empty,\n",
    "                empty,\n",
    "                semantics,\n",
    "                empty,\n",
    "            )  # Only semantics, used in eval, the same ``empty'' variable is a mistake in general cases but simple\n",
    "\n",
    "        # Predict spans\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        hop_start_logits, hop_end_logits, ans_start_logits, ans_end_logits = logits.split(\n",
    "            1, dim=-1\n",
    "        )\n",
    "        hop_start_logits = hop_start_logits.squeeze(-1)\n",
    "        hop_end_logits = hop_end_logits.squeeze(-1)\n",
    "        ans_start_logits = ans_start_logits.squeeze(-1)\n",
    "        ans_end_logits = ans_end_logits.squeeze(-1)  # Shape: [batch_size, max_length]\n",
    "\n",
    "        if hop_start_weights is not None:  # Train mode\n",
    "            lgsf = torch.nn.LogSoftmax(\n",
    "                dim=1\n",
    "            )  # If there is no targeted span in the sentence, start_weights = end_weights = 0(vec)\n",
    "            hop_start_loss = -torch.sum(\n",
    "                hop_start_weights * lgsf(hop_start_logits), dim=-1\n",
    "            )\n",
    "            hop_end_loss = -torch.sum(hop_end_weights * lgsf(hop_end_logits), dim=-1)\n",
    "            ans_start_loss = -torch.sum(\n",
    "                ans_start_weights * lgsf(ans_start_logits), dim=-1\n",
    "            )\n",
    "            ans_end_loss = -torch.sum(ans_end_weights * lgsf(ans_end_logits), dim=-1)\n",
    "            hop_loss = torch.mean((hop_start_loss + hop_end_loss)) / 2\n",
    "            ans_loss = torch.mean((ans_start_loss + ans_end_loss)) / 2\n",
    "        else:\n",
    "            # In eval mode, find the exact top K spans.\n",
    "            K_hop, K_ans = 3, 1\n",
    "            hop_preds = torch.zeros(\n",
    "                batch_size, K_hop, 3, dtype=torch.long, device=device\n",
    "            )  # (start, end, sen_num)\n",
    "            ans_preds = torch.zeros(\n",
    "                batch_size, K_ans, 3, dtype=torch.long, device=device\n",
    "            )\n",
    "            ans_start_gap = torch.zeros(batch_size, device=device)\n",
    "            for u, (start_logits, end_logits, preds, K, allow) in enumerate(\n",
    "                (\n",
    "                    (\n",
    "                        hop_start_logits,\n",
    "                        hop_end_logits,\n",
    "                        hop_preds,\n",
    "                        K_hop,\n",
    "                        allow_limit[0],\n",
    "                    ),\n",
    "                    (\n",
    "                        ans_start_logits,\n",
    "                        ans_end_logits,\n",
    "                        ans_preds,\n",
    "                        K_ans,\n",
    "                        allow_limit[1],\n",
    "                    ),\n",
    "                )\n",
    "            ):\n",
    "                for i in range(batch_size):\n",
    "                    if sep_positions[i, 0] > 0:\n",
    "                        values, indices = start_logits[i, B_starts[i] :].topk(K)\n",
    "                        for k, index in enumerate(indices):\n",
    "                            if values[k] <= start_logits[i, 0] - allow:  # not golden\n",
    "                                if u == 1: # For ans spans\n",
    "                                    ans_start_gap[i] = start_logits[i, 0] - values[k]\n",
    "                                break\n",
    "                            start = index + B_starts[i]\n",
    "                            # find ending\n",
    "                            for j, ending in enumerate(sep_positions[i]):\n",
    "                                if ending > start or ending <= 0:\n",
    "                                    break\n",
    "                            if ending <= start:\n",
    "                                break\n",
    "                            ending = min(ending, start + 10)\n",
    "                            end = torch.argmax(end_logits[i, start:ending]) + start\n",
    "                            preds[i, k, 0] = start\n",
    "                            preds[i, k, 1] = end\n",
    "                            preds[i, k, 2] = j\n",
    "        return (\n",
    "            (hop_loss, ans_loss, semantics)\n",
    "            if hop_start_weights is not None\n",
    "            else (hop_preds, ans_preds, semantics, ans_start_gap)\n",
    "        )\n",
    "\n",
    "\n",
    "class CognitiveGNN(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(CognitiveGNN, self).__init__()\n",
    "        self.gcn = GCN(hidden_size)\n",
    "        self.both_net = MLP((hidden_size, hidden_size, 1))\n",
    "        self.select_net = MLP((hidden_size, hidden_size, 1))\n",
    "\n",
    "    def forward(self, bundle, model, device):\n",
    "        batch = bundle_part_to_batch(bundle)\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        hop_loss, ans_loss, semantics = model(\n",
    "            *batch\n",
    "        )  # Shape of semantics: [num_para, hidden_size]\n",
    "        num_additional_nodes = len(bundle.additional_nodes)\n",
    "\n",
    "        if num_additional_nodes > 0:\n",
    "            max_length_additional = max([len(x) for x in bundle.additional_nodes])\n",
    "            ids = torch.zeros(\n",
    "                (num_additional_nodes, max_length_additional),\n",
    "                dtype=torch.long,\n",
    "                device=device,\n",
    "            )\n",
    "            segment_ids = torch.zeros(\n",
    "                (num_additional_nodes, max_length_additional),\n",
    "                dtype=torch.long,\n",
    "                device=device,\n",
    "            )\n",
    "            input_mask = torch.zeros(\n",
    "                (num_additional_nodes, max_length_additional),\n",
    "                dtype=torch.long,\n",
    "                device=device,\n",
    "            )\n",
    "            for i in range(num_additional_nodes):\n",
    "                length = len(bundle.additional_nodes[i])\n",
    "                ids[i, :length] = torch.tensor(\n",
    "                    bundle.additional_nodes[i], dtype=torch.long\n",
    "                )\n",
    "                input_mask[i, :length] = 1\n",
    "            additional_semantics = model(ids, segment_ids, input_mask)\n",
    "\n",
    "            semantics = torch.cat((semantics, additional_semantics), dim=0)\n",
    "\n",
    "        assert semantics.size()[0] == bundle.adj.size()[0]\n",
    "\n",
    "        if bundle.question_type == 0:  # Wh-\n",
    "            pred = self.gcn(bundle.adj.to(device), semantics)\n",
    "            ce = torch.nn.CrossEntropyLoss()\n",
    "            final_loss = ce(\n",
    "                pred.unsqueeze(0),\n",
    "                torch.tensor([bundle.answer_id], dtype=torch.long, device=device),\n",
    "            )\n",
    "        else:\n",
    "            x, y, ans = bundle.answer_id\n",
    "            ans = torch.tensor(ans, dtype=torch.float, device=device)\n",
    "            diff_sem = semantics[x] - semantics[y]\n",
    "            classifier = self.both_net if bundle.question_type == 1 else self.select_net\n",
    "            final_loss = 0.2 * torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "                classifier(diff_sem).squeeze(-1), ans.to(device)\n",
    "            )\n",
    "        return hop_loss, ans_loss, final_loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    BERT_MODEL = \"bert-base-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True)\n",
    "    orig_text = \"\".join(\n",
    "        [\n",
    "            \"Theatre Centre is a UK-based theatre company touring new plays for young audiences aged 4 to 18, founded in 1953 by Brian Way, the company has developed plays by writers including which British writer, dub poet and Rastafarian?\",\n",
    "            \" It is the largest urban not-for-profit theatre company in the country and the largest in Western Canada, with productions taking place at the 650-seat Stanley Industrial Alliance Stage, the 440-seat Granville Island Stage, the 250-seat Goldcorp Stage at the BMO Theatre Centre, and on tour around the province.\",\n",
    "        ]\n",
    "    )\n",
    "    tokenized_text = tokenizer.tokenize(orig_text)\n",
    "    print(len(tokenized_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "import pdb\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import copy\n",
    "import traceback\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from pytorch_pretrained_bert.tokenization import whitespace_tokenize, BasicTokenizer, BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForQuestionAnswering\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "\n",
    "\n",
    "\n",
    "class Bundle(object):\n",
    "    \"\"\"The structure to contain all data for training. \n",
    "    \n",
    "    A flexible class. The properties are defined in FIELDS and dynamically added by capturing variables with the same names at runtime.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "FIELDS = ['ids', 'hop_start_weights', 'hop_end_weights', 'ans_start_weights', 'ans_end_weights', 'segment_ids', 'sep_positions',\n",
    "     'additional_nodes', 'adj', 'answer_id', 'question_type', '_id']\n",
    "\n",
    "\n",
    "# Judge question type with interrogative words\n",
    "GENERAL_WD = ['is', 'are', 'am', 'was', 'were', 'have', 'has', 'had', 'can', 'could', \n",
    "              'shall', 'will', 'should', 'would', 'do', 'does', 'did', 'may', 'might', 'must', 'ought', 'need', 'dare']\n",
    "GENERAL_WD += [x.capitalize() for x in GENERAL_WD]\n",
    "GENERAL_WD = re.compile(' |'.join(GENERAL_WD))\n",
    "def judge_question_type(q : str, G = GENERAL_WD) -> int:\n",
    "    if q.find(' or ') >= 0:\n",
    "        return 2 \n",
    "    elif G.match(q):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def improve_question_type_and_answer(data, e2i):\n",
    "    '''Improve the result of the judgement of question type in training data with other information.\n",
    "    \n",
    "    If the question is a special question(type 0), answer_id is the index of final answer node. Otherwise answer_ids are\n",
    "    the indices of two compared nodes and the result of comparison(0 / 1).\n",
    "    This part is not very important to the overall results, but avoids Runtime Errors in rare cases.\n",
    "    \n",
    "    Args:\n",
    "        data (Json): Refined distractor-setting samples.\n",
    "        e2i (dict): entity2index dict.\n",
    "    \n",
    "    Returns:\n",
    "        (int, int or (int, int, 0 / 1), string): question_type, answer_id and answer_entity.\n",
    "    '''\n",
    "    question_type = judge_question_type(data['question'])\n",
    "    # fix judgement by answer\n",
    "    if data['answer'] == 'yes' or data['answer'] == 'no':\n",
    "        question_type = 1\n",
    "        answer_entity = data['answer']\n",
    "    else:\n",
    "        # check whether the answer can be extracted as a span\n",
    "        answer_entity = fuzzy_retrieve(data['answer'], e2i, 'distractor', 80)\n",
    "        if answer_entity is None:\n",
    "            raise ValueError('Cannot find answer: {}'.format(data['answer']))\n",
    "    \n",
    "    if question_type == 0:\n",
    "        answer_id = e2i[answer_entity]\n",
    "    elif len(data['Q_edge']) != 2:\n",
    "        if question_type == 1:\n",
    "            raise ValueError('There must be 2 entities in \"Q_edge\" for type 1 question.')\n",
    "        elif question_type == 2: # Judgement error, should be type 0\n",
    "            question_type = 0\n",
    "            answer_id = e2i[answer_entity]\n",
    "    else:\n",
    "        answer_id = [e2i[data['Q_edge'][0][0]], e2i[data['Q_edge'][1][0]]] # compared nodes\n",
    "        if question_type == 1:\n",
    "            answer_id.append(int(data['answer'] == 'yes'))\n",
    "        elif question_type == 2:\n",
    "            if data['answer'] == data['Q_edge'][0][1]:\n",
    "                answer_id.append(0)\n",
    "            elif data['answer'] == data['Q_edge'][1][1]:\n",
    "                answer_id.append(1)\n",
    "            else: # cannot exactly match an option\n",
    "                score = (fuzz.partial_ratio(data['answer'], data['Q_edge'][0][1]), fuzz.partial_ratio(data['answer'], data['Q_edge'][1][1]))\n",
    "                if score[0] < 50 and score[1] < 50:\n",
    "                    raise ValueError('There is no exact match in selecting question. answer: {}'.format(data['answer']))\n",
    "                else:\n",
    "                    answer_id.append(0 if score[0] > score[1] else 1)\n",
    "    return question_type, answer_id, answer_entity\n",
    "\n",
    "def convert_question_to_samples_bundle(tokenizer, data: 'Json refined', neg = 2):\n",
    "    '''Make training samples.\n",
    "    \n",
    "    Convert distractor-setting samples(question + 10 paragraphs + answer + supporting facts) to bundles.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer (BertTokenizer): BERT Tokenizer to transform sentences to a list of word pieces.\n",
    "        data (Json): Refined distractor-setting samples with gold-only cognitive graphs. \n",
    "        neg (int, optional): Defaults to 2. Negative answer nodes to add in every sample.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: Invalid question type. \n",
    "\n",
    "    Returns:\n",
    "        Bundle: A bundle containing 10 separate samples(including gold and negative samples).\n",
    "    '''\n",
    "\n",
    "    context = dict(data['context']) # all the entities in 10 paragraphs\n",
    "    gold_sentences_set = dict([((para, sen), edges) for para, sen, edges in data['supporting_facts']]) \n",
    "    e2i, i2e = {}, [] # entity2index, index2entity\n",
    "    for entity, sens in context.items():\n",
    "        e2i[entity] = len(i2e)\n",
    "        i2e.append(entity)\n",
    "    clues = [[]] * len(i2e) # pre-extracted clues\n",
    "\n",
    "    ids, hop_start_weights, hop_end_weights, ans_start_weights, ans_end_weights, segment_ids, sep_positions, additional_nodes = [], [], [], [], [], [], [], []\n",
    "    tokenized_question = ['[CLS]'] + tokenizer.tokenize(data['question']) + ['[SEP]']\n",
    "\n",
    "    # Extract clues for entities in the gold-only cogntive graph\n",
    "    for entity_x, sen, edges in data['supporting_facts']:\n",
    "        for entity_y, _, _, _ in edges:\n",
    "            if entity_y not in e2i: # entity y must be the answer\n",
    "                assert data['answer'] == entity_y\n",
    "                e2i[entity_y] = len(i2e)\n",
    "                i2e.append(entity_y)\n",
    "                clues.append([])\n",
    "            if entity_x != entity_y:\n",
    "                y = e2i[entity_y]\n",
    "                clues[y] = clues[y] + tokenizer.tokenize(context[entity_x][sen]) + ['[SEP]']\n",
    "    \n",
    "    question_type, answer_id, answer_entity = improve_question_type_and_answer(data, e2i)\n",
    "    \n",
    "    # Construct training samples\n",
    "    for entity, para in context.items():\n",
    "        num_hop, num_ans = 0, 0\n",
    "        tokenized_all = tokenized_question + clues[e2i[entity]]\n",
    "        if len(tokenized_all) > 512: # BERT-base accepts at most 512 tokens\n",
    "            tokenized_all = tokenized_all[:512]\n",
    "            print('CLUES TOO LONG, id: {}'.format(data['_id']))\n",
    "        # initialize a sample for ``entity''\n",
    "        sep_position = [] \n",
    "        segment_id = [0] * len(tokenized_all)\n",
    "        hop_start_weight = [0] * len(tokenized_all)\n",
    "        hop_end_weight = [0] * len(tokenized_all)\n",
    "        ans_start_weight = [0] * len(tokenized_all)\n",
    "        ans_end_weight = [0] * len(tokenized_all)\n",
    "\n",
    "        for sen_num, sen in enumerate(para):\n",
    "            tokenized_sen = tokenizer.tokenize(sen) + ['[SEP]']\n",
    "            if len(tokenized_all) + len(tokenized_sen) > 512 or sen_num > 15:\n",
    "                break\n",
    "            tokenized_all += tokenized_sen\n",
    "            segment_id += [sen_num + 1] * len(tokenized_sen)\n",
    "            sep_position.append(len(tokenized_all) - 1)\n",
    "            hs_weight = [0] * len(tokenized_sen)\n",
    "            he_weight = [0] * len(tokenized_sen)\n",
    "            as_weight = [0] * len(tokenized_sen)\n",
    "            ae_weight = [0] * len(tokenized_sen)\n",
    "            if (entity, sen_num) in gold_sentences_set:\n",
    "                edges = gold_sentences_set[(entity, sen_num)]\n",
    "                intervals = find_start_end_after_tokenized(tokenizer, tokenized_sen,\n",
    "                    [matched  for _, matched, _, _ in edges])\n",
    "                for j, (l, r) in enumerate(intervals):\n",
    "                    if edges[j][0] == answer_entity or question_type > 0: # successive node edges[j][0] is answer node\n",
    "                        as_weight[l] = ae_weight[r] = 1\n",
    "                        num_ans += 1\n",
    "                    else: # edges[j][0] is next-hop node\n",
    "                        hs_weight[l] = he_weight[r] = 1\n",
    "                        num_hop += 1\n",
    "            hop_start_weight += hs_weight\n",
    "            hop_end_weight += he_weight\n",
    "            ans_start_weight += as_weight\n",
    "            ans_end_weight += ae_weight\n",
    "            \n",
    "        assert len(tokenized_all) <= 512\n",
    "        # if entity is a negative node, train negative threshold at [CLS] \n",
    "        if 1 not in hop_start_weight:\n",
    "            hop_start_weight[0] = 0.1\n",
    "        if 1 not in ans_start_weight:\n",
    "            ans_start_weight[0] = 0.1\n",
    "\n",
    "        ids.append(tokenizer.convert_tokens_to_ids(tokenized_all))\n",
    "        sep_positions.append(sep_position)\n",
    "        segment_ids.append(segment_id)\n",
    "        hop_start_weights.append(hop_start_weight)\n",
    "        hop_end_weights.append(hop_end_weight)\n",
    "        ans_start_weights.append(ans_start_weight)\n",
    "        ans_end_weights.append(ans_end_weight)\n",
    "\n",
    "    # Construct negative answer nodes for task #2(answer node prediction)\n",
    "    n = len(context)\n",
    "    edges_in_bundle = []\n",
    "    if question_type == 0:\n",
    "        # find all edges and prepare forbidden set(containing answer) for negative sampling\n",
    "        forbidden = set([])\n",
    "        for para, sen, edges in data['supporting_facts']:\n",
    "            for x, matched, l, r in edges:\n",
    "                edges_in_bundle.append((e2i[para], e2i[x]))\n",
    "                if x == answer_entity:\n",
    "                    forbidden.add((para, sen))\n",
    "        if answer_entity not in context and answer_entity in e2i:\n",
    "            n += 1\n",
    "            tokenized_all = tokenized_question + clues[e2i[answer_entity]]\n",
    "            if len(tokenized_all) > 512:\n",
    "                tokenized_all = tokenized_all[:512]\n",
    "                print('ANSWER TOO LONG! id: {}'.format(data['_id']))\n",
    "            additional_nodes.append(tokenizer.convert_tokens_to_ids(tokenized_all))\n",
    "\n",
    "        for i in range(neg):\n",
    "            # build negative answer node n+i\n",
    "            father_para = random.choice(list(context.keys()))\n",
    "            father_sen = random.randrange(len(context[father_para]))\n",
    "            if (father_para, father_sen) in forbidden:\n",
    "                father_para = random.choice(list(context.keys()))\n",
    "                father_sen = random.randrange(len(context[father_para]))\n",
    "            if (father_para, father_sen) in forbidden:\n",
    "                neg -= 1\n",
    "                continue\n",
    "            tokenized_all = tokenized_question + tokenizer.tokenize(context[father_para][father_sen]) + ['[SEP]']\n",
    "            if len(tokenized_all) > 512:\n",
    "                tokenized_all = tokenized_all[:512]\n",
    "                print('NEG TOO LONG! id: {}'.format(data['_id']))\n",
    "            additional_nodes.append(tokenizer.convert_tokens_to_ids(tokenized_all))\n",
    "            edges_in_bundle.append((e2i[father_para], n))\n",
    "            n += 1\n",
    "\n",
    "    if question_type >= 1:\n",
    "        for para, sen, edges in data['supporting_facts']:\n",
    "            for x, matched, l, r in edges:\n",
    "                if e2i[para] < n and  e2i[x] < n:\n",
    "                    edges_in_bundle.append((e2i[para], e2i[x]))\n",
    "                    \n",
    "    assert n == len(additional_nodes) + len(context)\n",
    "    adj = torch.eye(n) * 2\n",
    "    for x, y in edges_in_bundle:\n",
    "        adj[x, y] = 1\n",
    "    adj /= torch.sum(adj, dim=0, keepdim=True)\n",
    "\n",
    "    _id = data['_id']\n",
    "    ret = Bundle()\n",
    "    for field in FIELDS:\n",
    "        setattr(ret, field, eval(field))\n",
    "    return ret\n",
    "    \n",
    "def homebrew_data_loader(bundles, mode : 'bundle or tensors' = 'tensors', batch_size = 8):\n",
    "    '''Return a generator like DataLoader in pytorch\n",
    "    \n",
    "    Different data are fed in task #1 and #2. In task #1, steps for different entities are decoupled into 10 samples\n",
    "    and can be randomly shuffled. But in task #2, inputs must be whole graphs. \n",
    "    \n",
    "    Args:\n",
    "        bundles (list): List of bundles for questions.\n",
    "        mode (string, optional): Defaults to 'tensors'. 'tensors' represents dataloader for task #1,\n",
    "            'bundle' represents dataloader for task #2.\n",
    "        batch_size (int, optional): Defaults to 8. \n",
    "    \n",
    "    Raises:\n",
    "        ValueError: Invalid mode\n",
    "    \n",
    "    Returns:\n",
    "        (int, Generator): number of batches and a generator to generate batches.\n",
    "    '''\n",
    "\n",
    "    if mode == 'bundle':\n",
    "        random.shuffle(bundles)\n",
    "        def gen():\n",
    "            for bundle in bundles:\n",
    "                yield bundle\n",
    "        return len(bundles), gen()\n",
    "    elif mode == 'tensors':\n",
    "        all_bundle = Bundle()\n",
    "        for field in FIELDS[:7]:\n",
    "            t = []\n",
    "            setattr(all_bundle, field, t)\n",
    "            for bundle in bundles:\n",
    "                t.extend(getattr(bundle, field))\n",
    "        n = len(t)\n",
    "        # random shuffle\n",
    "        orders = np.random.permutation(n)\n",
    "        for field in FIELDS[:7]:\n",
    "            t = getattr(all_bundle, field)\n",
    "            setattr(all_bundle, field, [t[x] for x in orders])\n",
    "        \n",
    "        num_batch = (n - 1) // batch_size + 1\n",
    "        def gen():\n",
    "            for batch_num in range(num_batch):\n",
    "                l, r = batch_num * batch_size, min((batch_num + 1) * batch_size, n)\n",
    "                yield bundle_part_to_batch(all_bundle, l, r)\n",
    "        return num_batch, gen()\n",
    "    else:\n",
    "        raise ValueError('mode must be \"bundle\" or \"tensors\"!')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Span Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "import pdb\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import traceback\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from pytorch_pretrained_bert.tokenization import whitespace_tokenize, BasicTokenizer, BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForQuestionAnswering\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "\n",
    "\n",
    "def train(bundles, model1, device, mode, model2, batch_size, num_epoch, gradient_accumulation_steps, lr1, lr2, alpha):\n",
    "    '''Train Sys1 and Sys2 models.\n",
    "    \n",
    "    Train models by task #1(tensors) and task #2(bundle). \n",
    "    \n",
    "    Args:\n",
    "        bundles (list): List of bundles.\n",
    "        model1 (BertForMultiHopQuestionAnswering): System 1 model.\n",
    "        device (torch.device): The device which models and data are on.\n",
    "        mode (str): Defaults to 'tensors'. Task identifier('tensors' or 'bundle').\n",
    "        model2 (CognitiveGNN): System 2 model.\n",
    "        batch_size (int): Defaults to 4.\n",
    "        num_epoch (int): Defaults to 1.\n",
    "        gradient_accumulation_steps (int): Defaults to 1. \n",
    "        lr1 (float): Defaults to 1e-4. Learning rate for Sys1.\n",
    "        lr2 (float): Defaults to 1e-4. Learning rate for Sys2.\n",
    "        alpha (float): Defaults to 0.2. Balance factor for loss of two systems.\n",
    "    \n",
    "    Returns:\n",
    "        ([type], [type]): Trained models.\n",
    "    '''\n",
    "\n",
    "    # Prepare optimizer for Sys1\n",
    "    param_optimizer = list(model1.named_parameters())\n",
    "    # hack to remove pooler, which is not used.\n",
    "    param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "    num_batch, dataloader = homebrew_data_loader(bundles, mode = mode, batch_size=batch_size)\n",
    "    num_steps = num_batch * num_epoch\n",
    "    global_step = 0\n",
    "    opt1 = BertAdam(optimizer_grouped_parameters, lr = lr1, warmup = 0.1, t_total=num_steps)\n",
    "    model1.to(device)\n",
    "    model1.train()\n",
    "\n",
    "    # Prepare optimizer for Sys2\n",
    "    if mode == 'bundle':\n",
    "        opt2 = Adam(model2.parameters(), lr=lr2)\n",
    "        model2.to(device)\n",
    "        model2.train()\n",
    "        warmed = False # warmup for jointly training\n",
    "\n",
    "    for epoch in trange(num_epoch, desc = 'Epoch'):\n",
    "        ans_mean, hop_mean = WindowMean(), WindowMean()\n",
    "        opt1.zero_grad()\n",
    "        if mode == 'bundle':\n",
    "            final_mean = WindowMean()\n",
    "            opt2.zero_grad()\n",
    "        tqdm_obj = tqdm(dataloader, total = num_batch)\n",
    "\n",
    "        for step, batch in enumerate(tqdm_obj):\n",
    "            try:\n",
    "                if mode == 'tensors':\n",
    "                    batch = tuple(t.to(device) for t in batch)\n",
    "                    hop_loss, ans_loss, pooled_output = model1(*batch)\n",
    "                    hop_loss, ans_loss = hop_loss.mean(), ans_loss.mean()\n",
    "                    pooled_output.detach()\n",
    "                    loss = ans_loss + hop_loss\n",
    "                elif mode == 'bundle':\n",
    "                    hop_loss, ans_loss, final_loss = model2(batch, model1, device)\n",
    "                    hop_loss, ans_loss = hop_loss.mean(), ans_loss.mean()\n",
    "                    loss = ans_loss + hop_loss + alpha * final_loss\n",
    "                loss.backward()\n",
    "\n",
    "                if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                    # modify learning rate with special warm up BERT uses. From BERT pytorch examples\n",
    "                    lr_this_step = lr1 * warmup_linear(global_step/num_steps, warmup = 0.1)\n",
    "                    for param_group in opt1.param_groups:\n",
    "                        param_group['lr'] = lr_this_step\n",
    "                    global_step += 1\n",
    "                    if mode == 'bundle':\n",
    "                        opt2.step()\n",
    "                        opt2.zero_grad()\n",
    "                        final_mean_loss = final_mean.update(final_loss.item())\n",
    "                        tqdm_obj.set_description('ans_loss: {:.2f}, hop_loss: {:.2f}, final_loss: {:.2f}'.format(\n",
    "                            ans_mean.update(ans_loss.item()), hop_mean.update(hop_loss.item()), final_mean_loss))\n",
    "                        # During warming period, model1 is frozen and model2 is trained to normal weights\n",
    "                        if final_mean_loss < 0.9 and step > 100: # ugly manual hyperparam\n",
    "                            warmed = True\n",
    "                        if warmed:\n",
    "                            opt1.step()\n",
    "                        opt1.zero_grad()\n",
    "                    else:\n",
    "                        opt1.step()\n",
    "                        opt1.zero_grad()\n",
    "                        tqdm_obj.set_description('ans_loss: {:.2f}, hop_loss: {:.2f}'.format(\n",
    "                            ans_mean.update(ans_loss.item()), hop_mean.update(hop_loss.item())))\n",
    "                    if step % 1000 == 0:\n",
    "                        output_model_file = './models/bert-base-uncased.bin.tmp'\n",
    "                        saved_dict = {'params1' : model1.module.state_dict()}\n",
    "                        saved_dict['params2'] = model2.state_dict()\n",
    "                        torch.save(saved_dict, output_model_file)\n",
    "            except Exception as err:\n",
    "                traceback.print_exc()\n",
    "                if mode == 'bundle':   \n",
    "                    print(batch._id) \n",
    "    return (model1, model2)\n",
    "\n",
    "\n",
    "def main(output_model_file = './models/bert-base-uncased.bin', load = False, mode = 'tensors', batch_size = 12, \n",
    "            num_epoch = 1, gradient_accumulation_steps = 1, lr1 = 1e-4, lr2 = 1e-4, alpha = 0.2):\n",
    "    \n",
    "    BERT_MODEL = 'bert-base-uncased' # bert-large is too large for ordinary GPU on task #2\n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True)\n",
    "    with open('./hotpot_train_v1.1_refined_1.json' ,'r') as fin:\n",
    "        dataset = json.load(fin)\n",
    "    bundles = []\n",
    "    for data in tqdm(dataset):\n",
    "        try:\n",
    "            bundles.append(convert_question_to_samples_bundle(tokenizer, data))\n",
    "        except ValueError as err:\n",
    "            pass\n",
    "        # except Exception as err:\n",
    "        #     traceback.print_exc()\n",
    "        #     pass\n",
    "    device = torch.device('cpu') if not torch.cuda.is_available() else torch.device('cuda')\n",
    "    if load:\n",
    "        print('Loading model from {}'.format(output_model_file))\n",
    "        model_state_dict = torch.load(output_model_file)\n",
    "        model1 = BertForMultiHopQuestionAnswering.from_pretrained(BERT_MODEL, state_dict=model_state_dict['params1'])\n",
    "        model2 = CognitiveGNN(model1.config.hidden_size)\n",
    "        model2.load_state_dict(model_state_dict['params2'])\n",
    "\n",
    "    else:\n",
    "        model1 = BertForMultiHopQuestionAnswering.from_pretrained(BERT_MODEL,\n",
    "                cache_dir=PYTORCH_PRETRAINED_BERT_CACHE / 'distributed_{}'.format(-1))\n",
    "        model2 = CognitiveGNN(model1.config.hidden_size)\n",
    "\n",
    "    print('Start Training... on {} GPUs'.format(torch.cuda.device_count()))\n",
    "    model1 = torch.nn.DataParallel(model1, device_ids = range(torch.cuda.device_count()))\n",
    "    model1, model2 = train(bundles, model1=model1, device=device, mode=mode, model2=model2, # Then pass hyperparams\n",
    "        batch_size=batch_size, num_epoch=num_epoch, gradient_accumulation_steps=gradient_accumulation_steps,lr1=lr1, lr2=lr2, alpha=alpha)\n",
    "    \n",
    "    print('Saving model to {}'.format(output_model_file))\n",
    "    saved_dict = {'params1' : model1.module.state_dict()}\n",
    "    saved_dict['params2'] = model2.state_dict()\n",
    "    torch.save(saved_dict, output_model_file)\n",
    "\n",
    "import fire\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Answer Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(load = True,mode = \"bundle\",batch_size = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pytorch_pretrained_bert.tokenization import whitespace_tokenize, BasicTokenizer, BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForQuestionAnswering\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm, trange\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "import pdb\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import copy\n",
    "# from line_profiler import LineProfiler\n",
    "\n",
    "def cognitive_graph_propagate(tokenizer, data: 'Json eval(Context as pool)', model1, model2, device, setting:'distractor / fullwiki' = 'fullwiki', max_new_nodes = 5):\n",
    "    \"\"\"Answer the question in ``data'' by trained CogQA model.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer (Tokenizer): Word-Piece tokenizer.\n",
    "        data (Json): Unrefined.\n",
    "        model1 (nn.Module): System 1 model.\n",
    "        model2 (nn.Module): System 2 model.\n",
    "        device (torch.device): Selected device.\n",
    "        setting (string, optional): 'distractor / fullwiki'. Defaults to 'fullwiki'.\n",
    "        max_new_nodes (int, optional): Maximum number of new nodes in cognitive graph. Defaults to 5.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (gold_ret, ans_ret, graph_ret, ans_nodes_ret)\n",
    "    \"\"\"\n",
    "    context = dict(data['context'])\n",
    "    e2i = dict([(entity, id) for id, entity in enumerate(context.keys())])\n",
    "    n = len(context)\n",
    "    i2e = [''] * n\n",
    "    for k, v in e2i.items():\n",
    "        i2e[v] = k  \n",
    "    prev = [[] for i in range(n)] # elements: (title, sen_num)\n",
    "    queue = range(n) \n",
    "    semantics = [None] * n\n",
    "\n",
    "    tokenized_question = ['[CLS]'] + tokenizer.tokenize(data['question']) + ['[SEP]']\n",
    "\n",
    "    def construct_infer_batch(queue):\n",
    "        \"\"\"Construct next batch (frontier nodes to visit).\n",
    "        \n",
    "        Args:\n",
    "            queue (list): A queue containing frontier nodes.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: A batch of inputs\n",
    "        \"\"\"\n",
    "        ids, sep_positions, segment_ids, tokenized_alls, B_starts = [], [], [], [], []\n",
    "        max_length, max_seps, num_samples = 0, 0, len(queue)\n",
    "        for x in queue:\n",
    "            tokenized_all = copy.copy(tokenized_question)\n",
    "            for title, sen_num in prev[x]:\n",
    "                tokenized_all += tokenizer.tokenize(context[title][sen_num]) + ['[SEP]']\n",
    "            if len(tokenized_all) > 512:\n",
    "                tokenized_all = tokenized_all[:512]\n",
    "                print('PREV TOO LONG, id: {}'.format(data['_id']))\n",
    "            segment_id = [0] * len(tokenized_all)\n",
    "            sep_position = [] \n",
    "            B_starts.append(len(tokenized_all))\n",
    "            for sen_num, sen in enumerate(context[i2e[x]]):\n",
    "                tokenized_sen = tokenizer.tokenize(sen) + ['[SEP]']\n",
    "                if len(tokenized_all) + len(tokenized_sen) > 512 or sen_num > 15:\n",
    "                    break\n",
    "                tokenized_all += tokenized_sen\n",
    "                segment_id += [sen_num + 1] * len(tokenized_sen)\n",
    "                sep_position.append(len(tokenized_all) - 1)\n",
    "            max_length = max(max_length, len(tokenized_all))\n",
    "            max_seps = max(max_seps, len(sep_position))\n",
    "            tokenized_alls.append(tokenized_all)\n",
    "            ids.append(tokenizer.convert_tokens_to_ids(tokenized_all))\n",
    "            sep_positions.append(sep_position)\n",
    "            segment_ids.append(segment_id)\n",
    "\n",
    "        ids_tensor = torch.zeros((num_samples, max_length), dtype = torch.long, device = device)\n",
    "        sep_positions_tensor = torch.zeros((num_samples, max_seps), dtype = torch.long, device = device)\n",
    "        segment_ids_tensor = torch.zeros((num_samples, max_length), dtype = torch.long, device = device)\n",
    "        input_mask = torch.zeros((num_samples, max_length), dtype = torch.long, device = device)\n",
    "        B_starts = torch.tensor(B_starts, dtype = torch.long, device = device)\n",
    "        for i in range(num_samples):\n",
    "            length = len(ids[i])\n",
    "            ids_tensor[i, :length] = torch.tensor(ids[i], dtype = torch.long)\n",
    "            sep_positions_tensor[i, :len(sep_positions[i])] = torch.tensor(sep_positions[i], dtype = torch.long)\n",
    "            segment_ids_tensor[i, :length] = torch.tensor(segment_ids[i], dtype = torch.long)\n",
    "            input_mask[i, :length] = 1\n",
    "        return ids_tensor, segment_ids_tensor, input_mask, sep_positions_tensor, tokenized_alls, B_starts\n",
    "    \n",
    "    gold_ret, ans_nodes = set([]), set([])\n",
    "    allow_limit = [0, 0]\n",
    "    while len(queue) > 0:\n",
    "        # visit all nodes in the frontier queue\n",
    "        ids, segment_ids, input_mask, sep_positions, tokenized_alls, B_starts = construct_infer_batch(queue)\n",
    "        hop_preds, ans_preds, semantics_preds, no_ans_logits = model1(ids, segment_ids, input_mask, sep_positions,\n",
    "            None, None, None, None, \n",
    "            B_starts, allow_limit)  \n",
    "        new_queue = []\n",
    "        for i, x in enumerate(queue):\n",
    "            semantics[x] = semantics_preds[i]\n",
    "            # for hop spans\n",
    "            for k in range(hop_preds.size()[1]):\n",
    "                l, r, j = hop_preds[i, k]\n",
    "                j = j.item()\n",
    "                if l == 0:\n",
    "                    break\n",
    "                gold_ret.add((i2e[x], j)) # supporting facts\n",
    "                orig_text = context[i2e[x]][j]\n",
    "                pred_slice = tokenized_alls[i][l : r + 1]\n",
    "                l, r = find_start_end_before_tokenized(orig_text, [pred_slice])[0]\n",
    "                if l == r == 0:\n",
    "                    continue    \n",
    "                recovered_matched = orig_text[l: r]\n",
    "                pool = context if setting == 'distractor' else (i2e[x], j)\n",
    "                matched = fuzzy_retrieve(recovered_matched, pool, setting)    \n",
    "                if matched is not None:\n",
    "                    if setting == 'fullwiki' and matched not in e2i and n < 10 + max_new_nodes:\n",
    "                        context_new = get_context_fullwiki(matched)\n",
    "                        if len(context_new) > 0: # cannot resovle redirection\n",
    "                            # create new nodes in the cognitive graph\n",
    "                            context[matched] = context_new\n",
    "                            prev.append([])\n",
    "                            semantics.append(None)\n",
    "                            e2i[matched] = n\n",
    "                            i2e.append(matched)\n",
    "                            n += 1\n",
    "                    if matched in e2i and e2i[matched] != x:\n",
    "                        y = e2i[matched]\n",
    "                        if y not in new_queue and (i2e[x], j) not in prev[y]:\n",
    "                            # new edge means new clues! update the successor as frontier nodes.\n",
    "                            new_queue.append(y)\n",
    "                            prev[y].append(((i2e[x], j)))\n",
    "            # for ans spans\n",
    "            for k in range(ans_preds.size()[1]):\n",
    "                l, r, j = ans_preds[i, k]\n",
    "                j = j.item()\n",
    "                if l == 0:\n",
    "                    break\n",
    "                gold_ret.add((i2e[x], j))\n",
    "                orig_text = context[i2e[x]][j]\n",
    "                pred_slice = tokenized_alls[i][l : r + 1]\n",
    "                l, r = find_start_end_before_tokenized(orig_text, [pred_slice])[0]\n",
    "                if l == r == 0:\n",
    "                    continue    \n",
    "                recovered_matched = orig_text[l: r]\n",
    "                matched = fuzzy_retrieve(recovered_matched, context, 'distractor', threshold=70)\n",
    "                if matched is not None:\n",
    "                    y = e2i[matched]\n",
    "                    ans_nodes.add(y)\n",
    "                    if (i2e[x], j) not in prev[y]:\n",
    "                        prev[y].append(((i2e[x], j)))\n",
    "                elif n < 10 + max_new_nodes:\n",
    "                    context[recovered_matched] = []\n",
    "                    e2i[recovered_matched] = n\n",
    "                    i2e.append(recovered_matched)\n",
    "                    new_queue.append(n)\n",
    "                    ans_nodes.add(n)\n",
    "                    prev.append([(i2e[x], j)])\n",
    "                    semantics.append(None)\n",
    "                    n += 1\n",
    "        if len(new_queue) == 0 and len(ans_nodes) == 0 and allow_limit[1] < 0.1: # must find one answer\n",
    "            # ``allow'' is an offset of negative threshold. \n",
    "            # If no ans span is valid, make the minimal gap between negative threshold and probability of ans spans -0.1, and try again.\n",
    "            prob, pos_in_queue = torch.min(no_ans_logits, dim = 0)\n",
    "            new_queue.append(queue[pos_in_queue])\n",
    "            allow_limit[1] = prob.item() + 0.1\n",
    "        queue = new_queue\n",
    "\n",
    "    question_type = judge_question_type(data['question'])\n",
    "\n",
    "    if n == 0:\n",
    "        return set([]), 'yes', [], []\n",
    "    if n == 1 and question_type > 0:\n",
    "        ans_ret = 'yes' if question_type == 1 else i2e[0]\n",
    "        return [(i2e[0], 0)], ans_ret, [], []\n",
    "    # GCN || CompareNets\n",
    "    semantics = torch.stack(semantics)\n",
    "    if question_type == 0:\n",
    "        adj = torch.eye(n, device = device) * 2\n",
    "        for x in range(n):\n",
    "            for title, sen_num in prev[x]:\n",
    "                adj[e2i[title], x] = 1\n",
    "        adj /= torch.sum(adj, dim=0, keepdim=True)\n",
    "        pred = model2.gcn(adj, semantics)\n",
    "        for x in range(n):\n",
    "            if x not in ans_nodes:\n",
    "                pred[x] -= 10000.\n",
    "        ans_ret = i2e[torch.argmax(pred).item()]\n",
    "    else:\n",
    "        # Take the most golden paragraphs as x,y\n",
    "        gold_num = torch.zeros(n)\n",
    "        for title, sen_num in gold_ret:\n",
    "            gold_num[e2i[title]] += 1\n",
    "        x, y = gold_num.topk(2)[1].tolist()\n",
    "        diff_sem = semantics[x] - semantics[y]\n",
    "        classifier = model2.both_net if question_type == 1 else model2.select_net\n",
    "        pred = int(torch.sigmoid(classifier(diff_sem)).item() > 0.5)\n",
    "        ans_ret = ['no', 'yes'][pred] if question_type == 1 else [i2e[x], i2e[y]][pred] \n",
    "    \n",
    "    ans_ret = re.sub(r' \\(.*?\\)$', '', ans_ret)\n",
    "\n",
    "    graph_ret = []\n",
    "    for x in range(n):\n",
    "        for title, sen_num in prev[x]:\n",
    "            graph_ret.append('({}, {}) --> {}'.format(title, sen_num, i2e[x]))    \n",
    "\n",
    "    ans_nodes_ret = [i2e[x] for x in ans_nodes]\n",
    "    return gold_ret, ans_ret, graph_ret, ans_nodes_ret\n",
    "\n",
    "def main(BERT_MODEL='bert-base-uncased', model_file='./models/bert-base-uncased.bin', data_file='./hotpot_dev_fullwiki_v1_merge.json', max_new_nodes=5):\n",
    "    setting = 'distractor' if data_file.find('distractor') >= 0 else 'fullwiki'\n",
    "    with open(data_file, 'r') as fin:\n",
    "        dataset = json.load(fin)\n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True)\n",
    "    device = torch.device('cpu') \n",
    "#     device = torch.device('cpu') if not torch.cuda.is_available() else torch.device('cuda')\n",
    "    print('Loading model from {}'.format(model_file))\n",
    "    model_state_dict = torch.load(model_file)\n",
    "    model1 = BertForMultiHopQuestionAnswering.from_pretrained(BERT_MODEL, state_dict=model_state_dict['params1'])\n",
    "    model2 = CognitiveGNN(model1.config.hidden_size)\n",
    "    model2.load_state_dict(model_state_dict['params2'])\n",
    "    sp, answer, graphs = {}, {}, {}\n",
    "    print('Start Training... on {} GPUs'.format(torch.cuda.device_count()))\n",
    "    model1 = torch.nn.DataParallel(model1, device_ids = range(torch.cuda.device_count()))\n",
    "    model1.to(device).eval()\n",
    "    model2.to(device).eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataset):\n",
    "            gold, ans, graph_ret, ans_nodes = cognitive_graph_propagate(tokenizer, data, model1, model2, device, setting = setting, max_new_nodes=max_new_nodes)\n",
    "            sp[data['_id']] = list(gold)\n",
    "            answer[data['_id']] = ans\n",
    "            graphs[data['_id']] = graph_ret + ['answer_nodes: ' + ', '.join(ans_nodes)]\n",
    "    pred_file = data_file.replace('.json', '_pred.json')\n",
    "    with open(pred_file, 'w') as fout:\n",
    "        json.dump({'answer': answer, 'sp': sp, 'graphs': graphs}, fout)\n",
    "    \n",
    "import fire\n",
    "if __name__ == \"__main__\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'em': 0.2922659862777795, 'f1': 0.3670088365378471, 'prec': 0.48387104854078733, 'recall': 0.3963252165799103, 'sp_em': 0.19636927285200542, 'sp_f1': 0.5167430116764522, 'sp_prec': 0.5502988632469649, 'sp_recall': 0.5692590852832177, 'joint_em': 0.10283703478544845, 'joint_f1': 0.27465057113200253, 'joint_prec': 0.38052119985128563, 'joint_recall': 0.290684510066337}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import ujson as json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "def normalize_answer(s):\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    normalized_prediction = normalize_answer(prediction)\n",
    "    normalized_ground_truth = normalize_answer(ground_truth)\n",
    "\n",
    "    ZERO_METRIC = (0, 0, 0)\n",
    "\n",
    "    if normalized_prediction in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "    if normalized_ground_truth in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return ZERO_METRIC\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def update_answer(metrics, prediction, gold):\n",
    "    em = exact_match_score(prediction, gold)\n",
    "    f1, prec, recall = f1_score(prediction, gold)\n",
    "    metrics['em'] += float(em)\n",
    "    metrics['f1'] += f1\n",
    "    metrics['prec'] += prec\n",
    "    metrics['recall'] += recall\n",
    "    return em, prec, recall\n",
    "\n",
    "def update_sp(metrics, prediction, gold):\n",
    "    cur_sp_pred = set(map(tuple, prediction))\n",
    "    gold_sp_pred = set(map(tuple, gold))\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for e in cur_sp_pred:\n",
    "        if e in gold_sp_pred:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    for e in gold_sp_pred:\n",
    "        if e not in cur_sp_pred:\n",
    "            fn += 1\n",
    "    prec = 1.0 * tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "    recall = 1.0 * tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "    f1 = 2 * prec * recall / (prec + recall) if prec + recall > 0 else 0.0\n",
    "    em = 1.0 if fp + fn == 0 else 0.0\n",
    "    metrics['sp_em'] += em\n",
    "    metrics['sp_f1'] += f1\n",
    "    metrics['sp_prec'] += prec\n",
    "    metrics['sp_recall'] += recall\n",
    "    return em, prec, recall\n",
    "\n",
    "def eval(prediction_file, gold_file):\n",
    "    with open(prediction_file) as f:\n",
    "        prediction = json.load(f)\n",
    "    with open(gold_file) as f:\n",
    "        gold = json.load(f)\n",
    "\n",
    "    metrics = {'em': 0, 'f1': 0, 'prec': 0, 'recall': 0,\n",
    "        'sp_em': 0, 'sp_f1': 0, 'sp_prec': 0, 'sp_recall': 0,\n",
    "        'joint_em': 0, 'joint_f1': 0, 'joint_prec': 0, 'joint_recall': 0}\n",
    "    for dp in gold:\n",
    "        cur_id = dp['_id']\n",
    "        can_eval_joint = True\n",
    "        if cur_id not in prediction['answer']:\n",
    "            print('missing answer {}'.format(cur_id))\n",
    "            can_eval_joint = False\n",
    "        else:\n",
    "            em, prec, recall = update_answer(\n",
    "                metrics, prediction['answer'][cur_id], dp['answer'])\n",
    "        if cur_id not in prediction['sp']:\n",
    "            print('missing sp fact {}'.format(cur_id))\n",
    "            can_eval_joint = False\n",
    "        else:\n",
    "            sp_em, sp_prec, sp_recall = update_sp(\n",
    "                metrics, prediction['sp'][cur_id], dp['supporting_facts'])\n",
    "\n",
    "        if can_eval_joint:\n",
    "            joint_prec = prec * sp_prec\n",
    "            joint_recall = recall * sp_recall\n",
    "            if joint_prec + joint_recall > 0:\n",
    "                joint_f1 = 2 * joint_prec * joint_recall / (joint_prec + joint_recall)\n",
    "            else:\n",
    "                joint_f1 = 0.\n",
    "            joint_em = em * sp_em\n",
    "\n",
    "            metrics['joint_em'] += joint_em\n",
    "            metrics['joint_f1'] += joint_f1\n",
    "            metrics['joint_prec'] += joint_prec\n",
    "            metrics['joint_recall'] += joint_recall\n",
    "\n",
    "    N = len(gold)\n",
    "    for k in metrics.keys():\n",
    "        metrics[k] /= N\n",
    "\n",
    "    print(metrics)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    eval(\"hotpot_dev_fullwiki_v1_pred_2020730.json\",\"hotpot_dev_fullwiki_v1_merge.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
