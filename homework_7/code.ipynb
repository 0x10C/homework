{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj shape (2708, 2708)\n",
      "feature shape (2708, 1433)\n",
      "label shape (2708, 7)\n",
      "train/validation/test split: 140/500/1000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "\n",
    "def load_data(dataset_str = 'cora'):\n",
    "    \"\"\"Load data.\"\"\"\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\"data/ind.{}.test.index\".format(dataset_str))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    # normalize\n",
    "    features = normalize(features)\n",
    "\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "        # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    # network_emb = pros(adj)\n",
    "    # network_emb = 0\n",
    "    \n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]     # onehot\n",
    "    \n",
    "    idx_train = range(len(y))       # training data index\n",
    "    idx_val = range(len(y), len(y)+500)     # validation data index\n",
    "    idx_test = test_idx_range.tolist()      # test data index \n",
    "\n",
    "    features = np.array(features.todense())\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    adj, features, labels, idx_train, idx_val, idx_test = load_data('cora')\n",
    "    print('adj shape', adj.shape)        # adjacency matrix with Shape(2708, 2708)\n",
    "    print('feature shape', features.shape)      # feature matrix, Shape(2708, 1433)\n",
    "    print('label shape', labels.shape)      #label matrix, Shape(2708, 7)\n",
    "    print('train/validation/test split: %s/%s/%s'%(len(idx_train), len(idx_val), len(idx_test)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from torch import LongTensor, Tensor\n",
    "class FeatureGraphDataset(object):\n",
    "\n",
    "    def __init__(self, features, label, adj):\n",
    "        '''Initalization\n",
    "        \n",
    "        Manually initalize a feature and graph dataset.\n",
    "\n",
    "        Args:\n",
    "            features: numpy ndarray, [[f1, f2, ...], [f1, f2, ...]]\n",
    "            label: numpy ndarray, [0, 1, 2, 0, ...], label[i] == -1 if its class is unknow\n",
    "            adj: dict of (int, list of int), {[1,2],[0,3],...}\n",
    "        '''\n",
    "        assert len(features) == len(label)\n",
    "        assert type(features) == type(label) == np.ndarray\n",
    "        self.features, self.label = features, label\n",
    "        self.n = len(features) # num of instances\n",
    "        self.m = np.max(label) + 1 # num of classes\n",
    "        self.k = features.shape[1] # num of features\n",
    "        self.adj = adj\n",
    "        ratio = 0.5\n",
    "        for k, v in adj.items():\n",
    "            s = len(v)\n",
    "            adj_features = reduce(lambda x,y: x + y, [self.features[y] for y in v])\n",
    "            self.features[k] = self.features[k] * ratio + adj_features * (1 - ratio) / s\n",
    "\n",
    "    def setting(self, label_num_per_class, test_num):\n",
    "        '''Set label data and test set in semi-supervised learning\n",
    "\n",
    "        Label data and test set should be settled at first. \n",
    "\n",
    "        '''\n",
    "        self.test_ids = random.sample(range(self.n), test_num)\n",
    "        remains = set(range(self.n)) - set(self.test_ids)\n",
    "        num_of_class = [0] * self.m\n",
    "        self.label_ids = []\n",
    "        for i in remains:\n",
    "            if num_of_class[self.label[i]] < label_num_per_class:\n",
    "                self.label_ids.append(i)\n",
    "            num_of_class[self.label[i]] += 1\n",
    "        self.unlabel_ids = list(set(range(self.n)) - set(self.label_ids))\n",
    "        self.test_num, self.label_num = test_num, sum(num_of_class)\n",
    "\n",
    "\n",
    "    def label_batch(self, batch_size, tensor = True):\n",
    "        '''Return a batch of label data features\n",
    "\n",
    "        Random sample from label data\n",
    "\n",
    "        Return:\n",
    "            tuple: ([id0, id1, ...], [[f1, f2, ...], ...(batch_size)](type: numpy.ndarray), [0,1,2,...(batch_size)](type: numpy.ndarray))\n",
    "        '''\n",
    "        assert(len(self.label_ids) >= batch_size)\n",
    "        ids = random.sample(self.label_ids, batch_size)\n",
    "        return (LongTensor(ids), Tensor(self.features[ids]), LongTensor(self.label[ids])) if tensor else (ids, self.features[ids], self.label[ids])\n",
    "    \n",
    "    def unlabel_batch(self, batch_size, tensor = True):\n",
    "        '''Return a batch of unlabel data features\n",
    "        \n",
    "        Random sample from label data\n",
    "\n",
    "        Return:\n",
    "            tuple: ([id0, ...], [[f1, f2, ...], ...(batch_size)](type: numpy.ndarray))\n",
    "        '''\n",
    "        if batch_size == -1:\n",
    "            ids = self.unlabel_ids\n",
    "        else:\n",
    "            ids = random.sample(self.unlabel_ids, batch_size)\n",
    "        return (LongTensor(ids), Tensor(self.features[ids])) if tensor else (ids, self.features[ids])\n",
    "\n",
    "    def test_batch(self, batch_size = -1, tensor = True):\n",
    "        if batch_size == -1:\n",
    "            ids = self.test_ids\n",
    "        else:\n",
    "            ids = random.sample(self.test_ids, batch_size)\n",
    "        return (LongTensor(ids), Tensor(self.features[ids]), LongTensor(self.label[ids])) if tensor else (ids, self.features[ids], self.label[ids])\n",
    "\n",
    "    def adj_batch(self, batch, tensor = True):\n",
    "        ids = [random.choice(self.adj[i]) for i in batch]\n",
    "        return (LongTensor(ids), Tensor(self.features[ids])) if tensor else (ids, self.features[ids])\n",
    "\n",
    "    def read_embbedings(self, embbeding_file):\n",
    "        '''read graph embbedings from file\n",
    "\n",
    "        Read graph embbedings generated by OpenNE system.        \n",
    "        '''\n",
    "        with open(embbeding_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            n, self.d = [int(i) for i in lines[0].split()]\n",
    "            assert n == self.n\n",
    "            self.embbedings = np.zeros((n, self.d))\n",
    "            for line in lines[1:]:\n",
    "                line = line.split()\n",
    "                self.embbedings[int(line[0])] = [float(i) for i in line[1:]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "def log_sum_exp(x, axis = 1):\n",
    "    m = torch.max(x, dim = 1)[0]\n",
    "    return m + torch.log(torch.sum(torch.exp(x - m.unsqueeze(1)), dim = axis))\n",
    "def normalize_infnorm(x, eps=1e-8):\n",
    "    assert type(x) == np.ndarray\n",
    "    return x / (abs(x).max(axis = 0) + 1e-8)   \n",
    "\n",
    "class LinearWeightNorm(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, weight_scale=None, weight_init_stdv=0.1):\n",
    "        super(LinearWeightNorm, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.randn(out_features, in_features) * weight_init_stdv)\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.zeros(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        if weight_scale is not None:\n",
    "            assert type(weight_scale) == int\n",
    "            self.weight_scale = Parameter(torch.ones(out_features, 1) * weight_scale)\n",
    "        else:\n",
    "            self.weight_scale = 1 \n",
    "    def forward(self, x):\n",
    "        W = self.weight * self.weight_scale / torch.sqrt(torch.sum(self.weight ** 2, dim = 1, keepdim = True))\n",
    "        return F.linear(x, W, self.bias)\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'in_features=' + str(self.in_features) \\\n",
    "            + ', out_features=' + str(self.out_features) +')'\n",
    "\n",
    "def pull_away_term(x):\n",
    "    '''pull-away loss\n",
    "\n",
    "    Args:\n",
    "        x: type=> torch Tensor or Variable, size=>[batch_size * feature_dim], generated samples\n",
    "\n",
    "    Return:\n",
    "        scalar Loss\n",
    "    '''\n",
    "    x = F.normalize(x)\n",
    "    pt = x.matmul(x.t()) ** 2\n",
    "    return (pt.sum() - pt.diag().sum()) / (len(x) * (len(x) - 1))\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "import pdb\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim = 28 ** 2, output_dim = 10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            LinearWeightNorm(input_dim, 500),\n",
    "            LinearWeightNorm(500, 500),\n",
    "            LinearWeightNorm(500, 250),\n",
    "            LinearWeightNorm(250, 250),\n",
    "            LinearWeightNorm(250, 250)]\n",
    "        )\n",
    "        self.final = LinearWeightNorm(250, output_dim, weight_scale=1)\n",
    "    def forward(self, x, feature = False, cuda = False, first = False):\n",
    "#        pdb.set_trace()\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        noise = torch.randn(x.size()) * 0.05 if self.training else torch.Tensor([0])\n",
    "        if cuda:\n",
    "            noise = noise.cuda()\n",
    "        x = x + Variable(noise, requires_grad = False)\n",
    "        if first:\n",
    "            return self.layers[0](x)\n",
    "        for i in range(len(self.layers)):\n",
    "            m = self.layers[i]\n",
    "            x_f = F.elu(m(x))\n",
    "            noise = torch.randn(x_f.size()) * 0.5 if self.training else torch.Tensor([0])\n",
    "            if cuda:\n",
    "                noise = noise.cuda()\n",
    "            x = (x_f + Variable(noise, requires_grad = False))\n",
    "        if feature:\n",
    "            return x_f, self.final(x)\n",
    "        return self.final(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, output_dim = 28 ** 2):\n",
    "        super(Generator, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.fc1 = nn.Linear(z_dim, 500, bias = False)\n",
    "        self.bn1 = nn.BatchNorm1d(500, affine = False, eps=1e-6, momentum = 0.5)\n",
    "        self.fc2 = nn.Linear(500, 500, bias = False)\n",
    "        self.bn2 = nn.BatchNorm1d(500, affine = False, eps=1e-6, momentum = 0.5)\n",
    "        self.fc3 = LinearWeightNorm(500, output_dim, weight_scale = 1)\n",
    "        self.bn1_b = Parameter(torch.zeros(500))\n",
    "        self.bn2_b = Parameter(torch.zeros(500))\n",
    "        nn.init.xavier_uniform(self.fc1.weight)\n",
    "        nn.init.xavier_uniform(self.fc2.weight)\n",
    "    def forward(self, batch_size, cuda = False, seed = -1):\n",
    "        x = Variable(torch.rand(batch_size, self.z_dim), requires_grad = False, volatile = not self.training)\n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "        x = F.elu(self.bn1(self.fc1(x)) + self.bn1_b)\n",
    "        x = F.elu(self.bn2(self.fc2(x)) + self.bn2_b)\n",
    "        x = F.tanh(self.fc3(x))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Generator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type LinearWeightNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Discriminator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1347: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 100 / 100\n",
      "Iteration 0, loss_supervised = 0.4530, loss_unsupervised = 0.5101, loss_gen = 1.0487 train acc = 0.8317\n",
      "Eval: correct 764 / 1000, Acc: 76.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:87: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 100 / 100\n",
      "Iteration 1, loss_supervised = 0.0400, loss_unsupervised = 0.2581, loss_gen = 1.3300 train acc = 0.9873\n",
      "Eval: correct 772 / 1000, Acc: 77.20\n",
      "Training: 100 / 100\n",
      "Iteration 2, loss_supervised = 0.0184, loss_unsupervised = 0.2929, loss_gen = 1.0697 train acc = 0.9945\n",
      "Eval: correct 787 / 1000, Acc: 78.70\n",
      "Training: 100 / 100\n",
      "Iteration 3, loss_supervised = 0.0181, loss_unsupervised = 0.3869, loss_gen = 0.5057 train acc = 0.9937\n",
      "Eval: correct 806 / 1000, Acc: 80.60\n",
      "Training: 100 / 100\n",
      "Iteration 4, loss_supervised = 0.0154, loss_unsupervised = 0.4276, loss_gen = 0.3555 train acc = 0.9956\n",
      "Eval: correct 829 / 1000, Acc: 82.90\n",
      "Training: 100 / 100\n",
      "Iteration 5, loss_supervised = 0.0142, loss_unsupervised = 0.4441, loss_gen = 0.3177 train acc = 0.9952\n",
      "Eval: correct 819 / 1000, Acc: 81.90\n",
      "Training: 100 / 100\n",
      "Iteration 6, loss_supervised = 0.0121, loss_unsupervised = 0.4460, loss_gen = 0.3101 train acc = 0.9958\n",
      "Eval: correct 816 / 1000, Acc: 81.60\n",
      "Training: 100 / 100\n",
      "Iteration 7, loss_supervised = 0.0105, loss_unsupervised = 0.4458, loss_gen = 0.3050 train acc = 0.9969\n",
      "Eval: correct 820 / 1000, Acc: 82.00\n",
      "Training: 100 / 100\n",
      "Iteration 8, loss_supervised = 0.0085, loss_unsupervised = 0.4436, loss_gen = 0.3157 train acc = 0.9978\n",
      "Eval: correct 833 / 1000, Acc: 83.30\n",
      "Training: 100 / 100\n",
      "Iteration 9, loss_supervised = 0.0067, loss_unsupervised = 0.4452, loss_gen = 0.3163 train acc = 0.9984\n",
      "Eval: correct 833 / 1000, Acc: 83.30\n",
      "Training: 100 / 100\n",
      "Iteration 10, loss_supervised = 0.0075, loss_unsupervised = 0.4360, loss_gen = 0.3286 train acc = 0.9980\n",
      "Eval: correct 833 / 1000, Acc: 83.30\n",
      "Training: 100 / 100\n",
      "Iteration 11, loss_supervised = 0.0066, loss_unsupervised = 0.4136, loss_gen = 0.3372 train acc = 0.9986\n",
      "Eval: correct 833 / 1000, Acc: 83.30\n",
      "Training: 100 / 100\n",
      "Iteration 12, loss_supervised = 0.0061, loss_unsupervised = 0.4136, loss_gen = 0.3470 train acc = 0.9984\n",
      "Eval: correct 835 / 1000, Acc: 83.50\n",
      "Training: 100 / 100\n",
      "Iteration 13, loss_supervised = 0.0068, loss_unsupervised = 0.4100, loss_gen = 0.3597 train acc = 0.9978\n",
      "Eval: correct 838 / 1000, Acc: 83.80\n",
      "Training: 100 / 100\n",
      "Iteration 14, loss_supervised = 0.0051, loss_unsupervised = 0.4176, loss_gen = 0.3647 train acc = 0.9989\n",
      "Eval: correct 836 / 1000, Acc: 83.60\n",
      "Training: 100 / 100\n",
      "Iteration 15, loss_supervised = 0.0045, loss_unsupervised = 0.4142, loss_gen = 0.3672 train acc = 0.9992\n",
      "Eval: correct 837 / 1000, Acc: 83.70\n",
      "Training: 100 / 100\n",
      "Iteration 16, loss_supervised = 0.0049, loss_unsupervised = 0.4257, loss_gen = 0.3700 train acc = 0.9989\n",
      "Eval: correct 841 / 1000, Acc: 84.10\n",
      "Training: 100 / 100\n",
      "Iteration 17, loss_supervised = 0.0060, loss_unsupervised = 0.4364, loss_gen = 0.3772 train acc = 0.9984\n",
      "Eval: correct 844 / 1000, Acc: 84.40\n",
      "Training: 100 / 100\n",
      "Iteration 18, loss_supervised = 0.0043, loss_unsupervised = 0.4487, loss_gen = 0.3679 train acc = 0.9994\n",
      "Eval: correct 843 / 1000, Acc: 84.30\n",
      "Training: 100 / 100\n",
      "Iteration 19, loss_supervised = 0.0061, loss_unsupervised = 0.4532, loss_gen = 0.3769 train acc = 0.9987\n",
      "Eval: correct 842 / 1000, Acc: 84.20\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "from __future__ import print_function \n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import sys\n",
    "from torch.nn.parameter import Parameter\n",
    "import argparse\n",
    "import tensorboardX\n",
    "import os\n",
    "import random\n",
    "import pickle as pkl\n",
    "class GraphSGAN(object):\n",
    "    def __init__(self, G, D, dataset, args):\n",
    "        if os.path.exists(args.savedir):\n",
    "            print('Loading model from ' + args.savedir)\n",
    "            self.G = torch.load(os.path.join(args.savedir, 'G.pkl'))\n",
    "            self.D = torch.load(os.path.join(args.savedir, 'D.pkl'))\n",
    "            self.embedding_layer = torch.load(os.path.join(args.savedir, 'embedding.pkl'))\n",
    "        else:\n",
    "            os.makedirs(args.savedir)\n",
    "            self.G = G\n",
    "            self.D = D\n",
    "            self.embedding_layer = nn.Embedding(dataset.n, dataset.d)\n",
    "            self.embedding_layer.weight = Parameter(torch.Tensor(dataset.embbedings))\n",
    "            torch.save(self.G, os.path.join(args.savedir, 'G.pkl'))\n",
    "            torch.save(self.D, os.path.join(args.savedir, 'D.pkl'))\n",
    "            torch.save(self.embedding_layer, os.path.join(args.savedir, 'embedding.pkl'))\n",
    "\n",
    "        self.writer = tensorboardX.SummaryWriter(log_dir=args.logdir)\n",
    "        if args.cuda:\n",
    "            self.G.cuda()\n",
    "            self.D.cuda() # self.embedding_layer is on CPU\n",
    "        self.dataset = dataset\n",
    "        self.Doptim = optim.Adam(self.D.parameters(), lr=args.lr, betas= (args.momentum, 0.999))\n",
    "        self.Goptim = optim.Adam(self.G.parameters(), lr=args.lr, betas = (args.momentum,0.999))\n",
    "        self.args = args\n",
    "\n",
    "    def trainD(self, idf_label, y, idf_unlabel):\n",
    "        x_label, x_unlabel, y = self.make_input(*idf_label), self.make_input(*idf_unlabel), Variable(y, requires_grad = False)\n",
    "        if self.args.cuda:\n",
    "            x_label, x_unlabel, y = x_label.cuda(), x_unlabel.cuda(), y.cuda()\n",
    "        output_label, (mom_un, output_unlabel), output_fake = self.D(x_label, cuda=self.args.cuda), self.D(x_unlabel, cuda=self.args.cuda, feature = True), self.D(self.G(x_unlabel.size()[0], cuda = self.args.cuda).view(x_unlabel.size()).detach(), cuda=self.args.cuda)\n",
    "        logz_label, logz_unlabel, logz_fake = log_sum_exp(output_label), log_sum_exp(output_unlabel), log_sum_exp(output_fake) # log ∑e^x_i\n",
    "        prob_label = torch.gather(output_label, 1, y.unsqueeze(1)) # log e^x_label = x_label \n",
    "        loss_supervised = -torch.mean(prob_label) + torch.mean(logz_label)\n",
    "        loss_unsupervised = 0.5 * (-torch.mean(logz_unlabel) + torch.mean(F.softplus(logz_unlabel))  + # real_data: log Z/(1+Z)\n",
    "                            torch.mean(F.softplus(logz_fake)) ) # fake_data: log 1/(1+Z)\n",
    "        entropy = -torch.mean(F.softmax(output_unlabel, dim = 1) * F.log_softmax(output_unlabel, dim = 1))\n",
    "        pt = pull_away_term(mom_un)\n",
    "        loss = loss_supervised + self.args.unlabel_weight * loss_unsupervised + entropy + pt\n",
    "        acc = torch.mean((output_label.max(1)[1] == y).float())\n",
    "        self.Doptim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.Doptim.step()\n",
    "        return loss_supervised.data.cpu().numpy(), loss_unsupervised.data.cpu().numpy(), acc\n",
    "    \n",
    "    def trainG(self, idf_unlabel):\n",
    "        x_unlabel = self.make_input(*idf_unlabel)\n",
    "        if self.args.cuda:\n",
    "            x_unlabel = x_unlabel.cuda()\n",
    "        fake = self.G(x_unlabel.size()[0], cuda = self.args.cuda).view(x_unlabel.size())\n",
    "        mom_gen, output_fake = self.D(fake, feature=True, cuda=self.args.cuda)\n",
    "        mom_unlabel, output_unlabel = self.D(x_unlabel, feature=True, cuda=self.args.cuda)\n",
    "        loss_pt = pull_away_term(mom_gen)\n",
    "        mom_gen = torch.mean(mom_gen, dim = 0)\n",
    "        mom_unlabel = torch.mean(mom_unlabel, dim = 0) \n",
    "        loss_fm = torch.mean(torch.abs(mom_gen - mom_unlabel))\n",
    "        loss = loss_fm + loss_pt \n",
    "        self.Goptim.zero_grad()\n",
    "        self.Doptim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.Goptim.step()\n",
    "        return loss.data.cpu().numpy()\n",
    "\n",
    "    def make_input(self, ids, feature, volatile = False):\n",
    "        '''Concatenate feature and embeddings\n",
    "\n",
    "        Args:\n",
    "            feature: Size=>[batch_size, dataset.k], Type=>FloatTensor\n",
    "            ids: Size=>[batch_size], Type=>LongTensor\n",
    "        '''\n",
    "        embedding = self.embedding_layer(Variable(ids, volatile = volatile)).detach() # detach temporarily\n",
    "        return torch.cat((Variable(feature), embedding), dim = 1)\n",
    "    def train(self):\n",
    "        gn = 0\n",
    "        NUM_BATCH = 100\n",
    "        for epoch in range(self.args.epochs):\n",
    "            self.G.train()\n",
    "            self.D.train()\n",
    "            self.D.turn = epoch\n",
    "            loss_supervised = loss_unsupervised = loss_gen = accuracy = 0.\n",
    "            for batch_num in range(NUM_BATCH):\n",
    "                # extract batch from dataset\n",
    "                idf_unlabel1 = self.dataset.unlabel_batch(self.args.batch_size)\n",
    "                idf_unlabel2 = self.dataset.unlabel_batch(self.args.batch_size)\n",
    "                id0, xf, y = self.dataset.label_batch(self.args.batch_size)\n",
    "\n",
    "                # train D\n",
    "                ll, lu, acc = self.trainD((id0, xf), y, idf_unlabel1)\n",
    "                loss_supervised += ll\n",
    "                loss_unsupervised += lu\n",
    "                accuracy += acc\n",
    "\n",
    "                # train G on unlabeled data\n",
    "                lg = self.trainG(idf_unlabel2)\n",
    "                loss_gen += lg\n",
    "                # print and record logs \n",
    "                if (batch_num + 1) % self.args.log_interval == 0:\n",
    "                    print('Training: %d / %d' % (batch_num + 1, NUM_BATCH))\n",
    "                    gn += 1\n",
    "                    self.writer.add_scalars('loss', {'loss_supervised':ll, 'loss_unsupervised':lu, 'loss_gen':lg}, gn)\n",
    "                    self.writer.add_histogram('real_feature', self.D(self.make_input(id0, xf, volatile = True).cuda(), cuda=self.args.cuda, feature = True)[0], gn)\n",
    "                    self.writer.add_histogram('fake_feature', self.D(self.G(self.args.batch_size, cuda = self.args.cuda), cuda=self.args.cuda, feature = True)[0], gn)\n",
    "            # calculate average loss at the end of an epoch\n",
    "            batch_num += 1\n",
    "            loss_supervised /= batch_num\n",
    "            loss_unsupervised /= batch_num\n",
    "            loss_gen /= batch_num\n",
    "            accuracy /= batch_num\n",
    "            print(\"Iteration %d, loss_supervised = %.4f, loss_unsupervised = %.4f, loss_gen = %.4f train acc = %.4f\" % (epoch, loss_supervised, loss_unsupervised, loss_gen, accuracy))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # eval\n",
    "            tmp = self.eval()\n",
    "            print(\"Eval: correct %d / %d, Acc: %.2f\"  % (tmp, self.dataset.test_num, tmp * 100. / self.dataset.test_num))\n",
    "            torch.save(self.G, os.path.join(self.args.savedir, 'G.pkl'))\n",
    "            torch.save(self.D, os.path.join(self.args.savedir, 'D.pkl'))\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        '''predict label in volatile mode\n",
    "\n",
    "        Args:\n",
    "            x: Size=>[batch_size, self.dataset.k + self.dataset.d], Type=>Variable(FloatTensor), volatile\n",
    "        '''\n",
    "        return torch.max(self.D(x, cuda=self.args.cuda), 1)[1].data\n",
    "\n",
    "    def eval(self):\n",
    "        self.G.eval()\n",
    "        self.D.eval()\n",
    "        ids, f, y = self.dataset.test_batch()\n",
    "        x = self.make_input(ids, f, volatile = True)\n",
    "        if self.args.cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "        pred1 = self.predict(x)\n",
    "\n",
    "        return torch.sum(pred1 == y)\n",
    "\n",
    "    def draw(self, batch_size):\n",
    "        self.G.eval()\n",
    "        return self.G(batch_size, cuda=self.args.cuda)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='PyTorch GraphS GAN')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--epochs', type=int, default=20, metavar='N',\n",
    "                        help='number of epochs to train (default: 20)')\n",
    "    parser.add_argument('--lr', type=float, default=0.003, metavar='LR',\n",
    "                        help='learning rate (default: 0.003)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                        help='SGD momentum (default: 0.5)')\n",
    "    parser.add_argument('--cuda', action='store_true', default=False,\n",
    "                        help='CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=2, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--eval-interval', type=int, default=1, metavar='N',\n",
    "                        help='how many batches to wait before evaling training status')\n",
    "    parser.add_argument('--unlabel-weight', type=float, default=0.5, metavar='N',\n",
    "                        help='scale factor between labeled and unlabeled data')\n",
    "    parser.add_argument('--logdir', type=str, default='./logfile', metavar='LOG_PATH', help='logfile path, tensorboard format')\n",
    "    parser.add_argument('--savedir', type=str, default='./models', metavar='SAVE_PATH', help = 'saving path, pickle format')\n",
    "    args = parser.parse_args([\"--cuda\"])\n",
    "    args.cuda = args.cuda and torch.cuda.is_available()\n",
    "    np.random.seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    # That is how you usually build the dataset \n",
    "    #dataset = CoraDataset(feature_file = './data/cora.features', \n",
    "    #     edge_file = './data/cora_edgelist', label_file = './data/cora_label')\n",
    "    #dataset.read_embbedings('./embedding/embedding_line_cora')\n",
    "    #dataset.setting(20, 1000)\n",
    "    \n",
    "\n",
    "    # but we load the example of cora\n",
    "    with open('cora.dataset/cora.dataset', 'rb') as fdata:\n",
    "        dataset = pkl.load(fdata,encoding='iso-8859-1')\n",
    "    gan = GraphSGAN(Generator(200, dataset.k + dataset.d), Discriminator(dataset.k + dataset.d, dataset.m), dataset, args)\n",
    "    gan.train() \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
